{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyPj2giZ1hJY/rh0EN1PsHOt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b8f7a28da0d047caad02beb459d72d6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7dcc236c2de74fabb459b74b8041a616","IPY_MODEL_699c9637e9e94c88a8ffb896d02ec404","IPY_MODEL_553b78796fbd4277bea08ff4dbd511ed"],"layout":"IPY_MODEL_e70c16a50b5741a89ad39d31e2026c02"}},"7dcc236c2de74fabb459b74b8041a616":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d1de17d8600430d97876e21190d0073","placeholder":"â€‹","style":"IPY_MODEL_f1188777294d44b6a51ab2441b5cdfb9","value":"Map: 100%"}},"699c9637e9e94c88a8ffb896d02ec404":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_720d403513bb4c1da4d571481f989796","max":287113,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1906be22b4dc4e718db03464545d0431","value":287113}},"553b78796fbd4277bea08ff4dbd511ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da3f388851c44dfa4ed98a2d3adcd0d","placeholder":"â€‹","style":"IPY_MODEL_d025e60654264142b2f60de94cefbb60","value":" 287113/287113 [06:35&lt;00:00, 709.15 examples/s]"}},"e70c16a50b5741a89ad39d31e2026c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d1de17d8600430d97876e21190d0073":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1188777294d44b6a51ab2441b5cdfb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"720d403513bb4c1da4d571481f989796":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1906be22b4dc4e718db03464545d0431":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7da3f388851c44dfa4ed98a2d3adcd0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d025e60654264142b2f60de94cefbb60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbbbddc320e54355ac22257ca84ff510":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0b595125e8b416b8ab491d5301aadab","IPY_MODEL_bde62acf72d74db49bcd5ec8c9ee108e","IPY_MODEL_f9c1904699df42ef9f789acb24bf84f2"],"layout":"IPY_MODEL_963ebf411897436a9e2edd23ccd5999a"}},"a0b595125e8b416b8ab491d5301aadab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1250aff5a4844c6082d43e5001faa23a","placeholder":"â€‹","style":"IPY_MODEL_178b1bb052cf4cf490aab42fb466efa2","value":"Map: 100%"}},"bde62acf72d74db49bcd5ec8c9ee108e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c9bc1015ceb41048927245d579569a3","max":1337,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a63e030cf62e46c183cd0185bc92ee54","value":1337}},"f9c1904699df42ef9f789acb24bf84f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d603d8f3e7c548b490d72f69a2501ea3","placeholder":"â€‹","style":"IPY_MODEL_ddf9e919a87e41a78f3582c435e33314","value":" 1337/1337 [00:01&lt;00:00, 790.38 examples/s]"}},"963ebf411897436a9e2edd23ccd5999a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1250aff5a4844c6082d43e5001faa23a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178b1bb052cf4cf490aab42fb466efa2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c9bc1015ceb41048927245d579569a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a63e030cf62e46c183cd0185bc92ee54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d603d8f3e7c548b490d72f69a2501ea3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddf9e919a87e41a78f3582c435e33314":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OgqpWMrPo5kK","executionInfo":{"status":"ok","timestamp":1701316876612,"user_tz":300,"elapsed":14140,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"outputs":[],"source":["%%capture\n","!pip install datasets\n","!pip install transformers\n","\n","import datasets as datasets\n","import transformers"]},{"cell_type":"code","source":["from transformers import BertTokenizerFast\n","\n","tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","tokenizer.bos_token = tokenizer.cls_token\n","tokenizer.eos_token = tokenizer.sep_token\n","\n","train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n","val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")"],"metadata":{"id":"2Trk26T_pEjI","executionInfo":{"status":"ok","timestamp":1701316879922,"user_tz":300,"elapsed":3316,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["batch_size=16  # change to 16 for full training #4\n","encoder_max_length=512\n","decoder_max_length=128\n","\n","def process_data_to_model_inputs(batch):\n","  # tokenize the inputs and labels\n","  inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n","  outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n","\n","  batch[\"input_ids\"] = inputs.input_ids\n","  batch[\"attention_mask\"] = inputs.attention_mask\n","  batch[\"decoder_input_ids\"] = outputs.input_ids\n","  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n","  batch[\"labels\"] = outputs.input_ids.copy()\n","\n","  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n","  # We have to make sure that the PAD token is ignored\n","  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n","\n","  return batch\n","\n","# only use 32 training examples for notebook - DELETE LINE FOR FULL TRAINING\n","#train_data = train_data.select(range(32))\n","\n","train_data = train_data.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n","    remove_columns=[\"article\", \"highlights\", \"id\"]\n",")\n","train_data.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",")\n","\n","\n","# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n","#val_data = val_data.select(range(16))\n","\n","val_data = val_data.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n","    remove_columns=[\"article\", \"highlights\", \"id\"]\n",")\n","val_data.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["b8f7a28da0d047caad02beb459d72d6e","7dcc236c2de74fabb459b74b8041a616","699c9637e9e94c88a8ffb896d02ec404","553b78796fbd4277bea08ff4dbd511ed","e70c16a50b5741a89ad39d31e2026c02","3d1de17d8600430d97876e21190d0073","f1188777294d44b6a51ab2441b5cdfb9","720d403513bb4c1da4d571481f989796","1906be22b4dc4e718db03464545d0431","7da3f388851c44dfa4ed98a2d3adcd0d","d025e60654264142b2f60de94cefbb60","fbbbddc320e54355ac22257ca84ff510","a0b595125e8b416b8ab491d5301aadab","bde62acf72d74db49bcd5ec8c9ee108e","f9c1904699df42ef9f789acb24bf84f2","963ebf411897436a9e2edd23ccd5999a","1250aff5a4844c6082d43e5001faa23a","178b1bb052cf4cf490aab42fb466efa2","5c9bc1015ceb41048927245d579569a3","a63e030cf62e46c183cd0185bc92ee54","d603d8f3e7c548b490d72f69a2501ea3","ddf9e919a87e41a78f3582c435e33314"]},"id":"UK3SK5jZpHbe","executionInfo":{"status":"ok","timestamp":1701317277302,"user_tz":300,"elapsed":397394,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}},"outputId":"44405772-6d1c-47c3-98bd-efc844efa327"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f7a28da0d047caad02beb459d72d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1337 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbbbddc320e54355ac22257ca84ff510"}},"metadata":{}}]},{"cell_type":"code","source":["import warnings\n","\n","# Suppress specific future warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","# Suppress all future warnings\n","warnings.filterwarnings('ignore', category=FutureWarning)\n"],"metadata":{"id":"3QSeLWq7ZXGZ","executionInfo":{"status":"ok","timestamp":1701317312140,"user_tz":300,"elapsed":546,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from transformers import EncoderDecoderModel\n","\n","bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")"],"metadata":{"id":"jWaFeywXpNG0","executionInfo":{"status":"ok","timestamp":1701317281471,"user_tz":300,"elapsed":4173,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# set special tokens\n","bert2bert.config.decoder_start_token_id = tokenizer.bos_token_id\n","bert2bert.config.eos_token_id = tokenizer.eos_token_id\n","bert2bert.config.pad_token_id = tokenizer.pad_token_id\n","\n","# sensible parameters for beam search\n","bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n","bert2bert.config.max_length = 142\n","bert2bert.config.min_length = 56\n","bert2bert.config.no_repeat_ngram_size = 3\n","bert2bert.config.early_stopping = True\n","bert2bert.config.length_penalty = 2.0\n","bert2bert.config.num_beams = 4"],"metadata":{"id":"puNC6ppRtlZp","executionInfo":{"status":"ok","timestamp":1701317281471,"user_tz":300,"elapsed":23,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install git-python==1.0.3\n","!pip install sacrebleu==1.4.12\n","!pip install rouge_score\n","\n","from seq2seq_trainer import Seq2SeqTrainer\n","from transformers import TrainingArguments\n","from dataclasses import dataclass, field\n","from typing import Optional"],"metadata":{"id":"7xOKB6vjttPK","executionInfo":{"status":"ok","timestamp":1701317296749,"user_tz":300,"elapsed":15300,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["@dataclass\n","class Seq2SeqTrainingArguments(TrainingArguments):\n","    label_smoothing: Optional[float] = field(\n","        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n","    )\n","    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n","    predict_with_generate: bool = field(\n","        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n","    )\n","    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n","    encoder_layerdrop: Optional[float] = field(\n","        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n","    )\n","    decoder_layerdrop: Optional[float] = field(\n","        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n","    )\n","    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n","    attention_dropout: Optional[float] = field(\n","        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n","    )\n","    lr_scheduler: Optional[str] = field(\n","        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n","    )"],"metadata":{"id":"shQQGC7buWN2","executionInfo":{"status":"ok","timestamp":1701317296749,"user_tz":300,"elapsed":25,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# load rouge for validation\n","rouge = datasets.load_metric(\"rouge\")\n","\n","def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    # all unnecessary tokens are removed\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n","    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n","\n","    return {\n","        \"rouge2_precision\": round(rouge_output.precision, 4),\n","        \"rouge2_recall\": round(rouge_output.recall, 4),\n","        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n","    }"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5zY_v0VJuYsH","executionInfo":{"status":"ok","timestamp":1701317297946,"user_tz":300,"elapsed":1221,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}},"outputId":"16265f21-ac26-4164-d4f6-006c69799685"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-4324511b13f7>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  rouge = datasets.load_metric(\"rouge\")\n"]}]},{"cell_type":"code","source":["import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n","\n","# set training arguments - these params are not really tuned, feel free to change\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./\",\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    predict_with_generate=True,\n","    evaluation_strategy=\"steps\",  # or \"epoch\"\n","    do_train=True,\n","    do_eval=True,\n","    logging_steps=1000,  # set to 1000 for full training 2\n","    save_steps=500,  # set to 500 for full training 16\n","    eval_steps=8000,  # set to 8000 for full training 4\n","    warmup_steps=2000,  # set to 2000 for full training 1\n","    #max_steps=16, # delete for full training\n","    overwrite_output_dir=True,\n","    save_total_limit=3,\n","    fp16=True,\n","    local_rank=-1,\n",")\n","\n","# instantiate trainer\n","trainer = Seq2SeqTrainer(\n","    model=bert2bert,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_data,\n","    eval_dataset=val_data,\n",")\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":476},"id":"j4-1F1HnudcR","outputId":"2bdcf382-7b56-4c5f-f4e4-2cb898d98b00","executionInfo":{"status":"ok","timestamp":1701340133519,"user_tz":300,"elapsed":14386469,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":13,"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='20050' max='53835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [20050/53835 2:17:44 < 3:52:07, 2.43 it/s, Epoch 1.12/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge2 Precision</th>\n","      <th>Rouge2 Recall</th>\n","      <th>Rouge2 Fmeasure</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.022600</td>\n","      <td>0.001108</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.000000</td>\n","      <td>0.000001</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='53835' max='53835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [53835/53835 6:17:30, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge2 Precision</th>\n","      <th>Rouge2 Recall</th>\n","      <th>Rouge2 Fmeasure</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.022600</td>\n","      <td>0.001108</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.000000</td>\n","      <td>0.000001</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>0.000000</td>\n","      <td>0.000011</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000100</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000200</td>\n","      <td>0.000100</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000100</td>\n","      <td>0.000100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=53835, training_loss=0.008969762984575962, metrics={'train_runtime': 22651.122, 'train_samples_per_second': 38.026, 'train_steps_per_second': 2.377, 'total_flos': 5.283919414533427e+17, 'train_loss': 0.008969762984575962, 'epoch': 3.0})"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# from transformers import EncoderDecoderModel, BertTokenizer\n","\n","# # Load the tokenizer\n","# tokenizer = BertTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n","\n","# # Load the model\n","# model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n"],"metadata":{"id":"DqQhlI3RXBWG","executionInfo":{"status":"aborted","timestamp":1701317306226,"user_tz":300,"elapsed":6,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","from transformers import BertTokenizer, EncoderDecoderModel\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = EncoderDecoderModel.from_pretrained(\"./checkpoint-16\")\n","model.to(\"cuda\")\n","\n","test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n","\n","# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n","test_data = test_data.select(range(16))\n","\n","batch_size = 16  # change to 64 for full evaluation\n","\n","# map data correctly\n","def generate_summary(batch):\n","    # Tokenizer will automatically set [BOS] <text> [EOS]\n","    # cut off at BERT max length 512\n","    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n","    input_ids = inputs.input_ids.to(\"cuda\")\n","    attention_mask = inputs.attention_mask.to(\"cuda\")\n","\n","    outputs = model.generate(input_ids, attention_mask=attention_mask)\n","\n","    # all special tokens including will be removed\n","    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    batch[\"pred\"] = output_str\n","\n","    return batch\n","\n","results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n","\n","pred_str = results[\"pred\"]\n","label_str = results[\"highlights\"]\n","\n","rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n","\n","print(rouge_output)"],"metadata":{"id":"OY7mrh5iBEV7","colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"status":"error","timestamp":1701348367283,"user_tz":300,"elapsed":11,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}},"outputId":"9c1e41d0-7c76-46d2-e502-c736fba995d7"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-583c157bce9c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoderDecoderModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoderModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./checkpoint-16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# Your new text\n","new_text = \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n","bert2bert.to(\"cpu\")\n","# Tokenize the input text\n","input_ids = tokenizer.encode(new_text, return_tensors=\"pt\", max_length=512, truncation=True)\n","# Generate the summary\n","summary_ids = bert2bert.generate(input_ids, num_beams=4, max_length=128, early_stopping=True)\n","\n","# Decode the summary\n","summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","print(summary)"],"metadata":{"id":"SIAGRyaVBent","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"error","timestamp":1701348390847,"user_tz":300,"elapsed":608,"user":{"displayName":"Krishna Raketla","userId":"01345373877883518712"}},"outputId":"16734ddd-3b4e-4898-8c0f-59fc3eff7804"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b12cde35b1f3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Your new text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnew_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbert2bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tokenize the input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'bert2bert' is not defined"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"E1c5cUFyR2UT"},"execution_count":null,"outputs":[]}]}