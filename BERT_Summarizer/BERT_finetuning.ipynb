{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d74df23e3c64ec08f7b4f55d4242b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size=4  # change to 16 for full training\n",
    "encoder_max_length=512\n",
    "decoder_max_length=128\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "  inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch[\"input_ids\"] = inputs.input_ids\n",
    "  batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "  batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "  return batch\n",
    "\n",
    "# only use 32 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "train_data = train_data.select(range(32))\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "\n",
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "val_data = val_data.select(range(16))\n",
    "\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.eos_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n",
    "bert2bert.config.max_length = 142\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# %rm seq2seq_trainer.py\n",
    "# %wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py\n",
    "\n",
    "# %pip install git-python==1.0.3\n",
    "# %pip install sacrebleu==1.4.12\n",
    "# %pip install rouge_score\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Seq2SeqTrainingArguments(TrainingArguments):\n",
    "    label_smoothing: Optional[float] = field(\n",
    "        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n",
    "    )\n",
    "    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSamler or not.\"})\n",
    "    predict_with_generate: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n",
    "    )\n",
    "    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n",
    "    encoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    decoder_layerdrop: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n",
    "    attention_dropout: Optional[float] = field(\n",
    "        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n",
    "    )\n",
    "    lr_scheduler: Optional[str] = field(\n",
    "        default=\"linear\", metadata={\"help\": f\"Which lr scheduler to use.\"}\n",
    "    )\n",
    "    generation_config: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 1162, in init\n",
      "    wi.setup(kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 306, in setup\n",
      "    wandb_login._login(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 298, in _login\n",
      "    wlogin.prompt_api_key()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 221, in prompt_api_key\n",
      "    key, status = self._prompt_api_key()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py\", line 201, in _prompt_api_key\n",
      "    key = apikey.prompt_api_key(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/lib/apikey.py\", line 144, in prompt_api_key\n",
      "    key = input_callback(api_ask).strip()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/click/termui.py\", line 164, in prompt\n",
      "    value = prompt_func(prompt)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/click/termui.py\", line 147, in prompt_func\n",
      "    raise Abort() from None\n",
      "click.exceptions.Abort\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "An unexpected error occurred",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:1162\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1161\u001b[0m wi \u001b[39m=\u001b[39m _WandbInit()\n\u001b[0;32m-> 1162\u001b[0m wi\u001b[39m.\u001b[39;49msetup(kwargs)\n\u001b[1;32m   1163\u001b[0m \u001b[39massert\u001b[39;00m wi\u001b[39m.\u001b[39msettings\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:306\u001b[0m, in \u001b[0;36m_WandbInit.setup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_offline \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m settings\u001b[39m.\u001b[39m_noop:\n\u001b[0;32m--> 306\u001b[0m     wandb_login\u001b[39m.\u001b[39;49m_login(\n\u001b[1;32m    307\u001b[0m         anonymous\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39manonymous\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    308\u001b[0m         force\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mforce\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    309\u001b[0m         _disable_warning\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    310\u001b[0m         _silent\u001b[39m=\u001b[39;49msettings\u001b[39m.\u001b[39;49mquiet \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49msilent,\n\u001b[1;32m    311\u001b[0m         _entity\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mentity\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mor\u001b[39;49;00m settings\u001b[39m.\u001b[39;49mentity,\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[39m# apply updated global state after login was handled\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py:298\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m key:\n\u001b[0;32m--> 298\u001b[0m     wlogin\u001b[39m.\u001b[39;49mprompt_api_key()\n\u001b[1;32m    300\u001b[0m \u001b[39m# make sure login credentials get to the backend\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py:221\u001b[0m, in \u001b[0;36m_WandbLogin.prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprompt_api_key\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 221\u001b[0m     key, status \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prompt_api_key()\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m==\u001b[39m ApiKeyStatus\u001b[39m.\u001b[39mNOTTY:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_login.py:201\u001b[0m, in \u001b[0;36m_WandbLogin._prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     key \u001b[39m=\u001b[39m apikey\u001b[39m.\u001b[39;49mprompt_api_key(\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings,\n\u001b[1;32m    203\u001b[0m         api\u001b[39m=\u001b[39;49mapi,\n\u001b[1;32m    204\u001b[0m         no_offline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    205\u001b[0m         no_create\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings\u001b[39m.\u001b[39;49mforce \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_settings \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    208\u001b[0m     \u001b[39m# invalid key provided, try again\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/lib/apikey.py:144\u001b[0m, in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    141\u001b[0m     wandb\u001b[39m.\u001b[39mtermlog(\n\u001b[1;32m    142\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou can find your API key in your browser here: \u001b[39m\u001b[39m{\u001b[39;00mapp_url\u001b[39m}\u001b[39;00m\u001b[39m/authorize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n\u001b[0;32m--> 144\u001b[0m     key \u001b[39m=\u001b[39m input_callback(api_ask)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    145\u001b[0m write_key(settings, key, api\u001b[39m=\u001b[39mapi)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/click/termui.py:164\u001b[0m, in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     value \u001b[39m=\u001b[39m prompt_func(prompt)\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m value:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/click/termui.py:147\u001b[0m, in \u001b[0;36mprompt.<locals>.prompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    146\u001b[0m     echo(\u001b[39mNone\u001b[39;00m, err\u001b[39m=\u001b[39merr)\n\u001b[0;32m--> 147\u001b[0m \u001b[39mraise\u001b[39;00m Abort() \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mAbort\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/krishna/Desktop/Study/NLP Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna/Desktop/Study/NLP%20Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# instantiate trainer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna/Desktop/Study/NLP%20Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna/Desktop/Study/NLP%20Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     model\u001b[39m=\u001b[39mbert2bert,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna/Desktop/Study/NLP%20Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna/Desktop/Study/NLP%20Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_data,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krishna/Desktop/Study/NLP%20Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krishna/Desktop/Study/NLP%20Assignments/Course_Project/BERT_Summarizer/BERT_finetuning.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/transformers/trainer.py:1530\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1531\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1532\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1533\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1534\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1535\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/transformers/trainer.py:1762\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step\n\u001b[1;32m   1760\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m-> 1762\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallback_handler\u001b[39m.\u001b[39;49mon_train_begin(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontrol)\n\u001b[1;32m   1764\u001b[0m \u001b[39m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args\u001b[39m.\u001b[39mignore_data_skip:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/transformers/trainer_callback.py:366\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_begin\u001b[39m(\u001b[39mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    365\u001b[0m     control\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_event(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_begin\u001b[39;49m\u001b[39m\"\u001b[39;49m, args, state, control)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/transformers/trainer_callback.py:410\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_event\u001b[39m(\u001b[39mself\u001b[39m, event, args, state, control, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    409\u001b[0m     \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 410\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(callback, event)(\n\u001b[1;32m    411\u001b[0m             args,\n\u001b[1;32m    412\u001b[0m             state,\n\u001b[1;32m    413\u001b[0m             control,\n\u001b[1;32m    414\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    415\u001b[0m             tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m    416\u001b[0m             optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer,\n\u001b[1;32m    417\u001b[0m             lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler,\n\u001b[1;32m    418\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    419\u001b[0m             eval_dataloader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_dataloader,\n\u001b[1;32m    420\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    421\u001b[0m         )\n\u001b[1;32m    422\u001b[0m         \u001b[39m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/transformers/integrations/integration_utils.py:769\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     args\u001b[39m.\u001b[39mrun_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(args, state, model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/transformers/integrations/integration_utils.py:742\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m         init_args[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mrun_name\n\u001b[1;32m    741\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb\u001b[39m.\u001b[39mrun \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 742\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wandb\u001b[39m.\u001b[39;49minit(\n\u001b[1;32m    743\u001b[0m         project\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mWANDB_PROJECT\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mhuggingface\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    744\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_args,\n\u001b[1;32m    745\u001b[0m     )\n\u001b[1;32m    746\u001b[0m \u001b[39m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wandb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mupdate(combined_dict, allow_val_change\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:1204\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1202\u001b[0m             wandb\u001b[39m.\u001b[39mtermerror(\u001b[39m\"\u001b[39m\u001b[39mAbnormal program exit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1203\u001b[0m             os\u001b[39m.\u001b[39m_exit(\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1204\u001b[0m         \u001b[39mraise\u001b[39;00m Error(\u001b[39m\"\u001b[39m\u001b[39mAn unexpected error occurred\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror_seen\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "\u001b[0;31mError\u001b[0m: An unexpected error occurred"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",  # or \"epoch\"\n",
    "    logging_steps=2,  # set to 1000 for full training\n",
    "    save_steps=16,  # set to 500 for full training\n",
    "    eval_steps=4,  # set to 8000 for full training\n",
    "    warmup_steps=1,  # set to 2000 for full training\n",
    "    max_steps=16,  # delete for full training\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=False, \n",
    "    local_rank=-1,  # Ensures non-distributed training\n",
    "    # other parameters as needed\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
